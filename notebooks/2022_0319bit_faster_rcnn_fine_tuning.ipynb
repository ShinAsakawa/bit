{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/bit/blob/main/notebooks/2022_0319bit_faster_rcnn_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K27wlJLhj2R"
      },
      "source": [
        "---\n",
        "- source: https://www.kaggle.com/yerramvarun/fine-tuning-faster-rcnn-using-pytorch/notebook\n",
        "- date: 2022_0319\n",
        "- filename: 2022_0319bit_faster-rcnn_fine_tuning.ipynb\n",
        "---\n",
        "\n",
        "**注**: workers=0 で動作するので時間がかかる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPfHEd7Chj2V"
      },
      "source": [
        "# faster-rcnn 転移学習または微調整を用いた BIT 線分 2 等分課題\n",
        "\n",
        "[BIT] 図版を [Faster RCNN](https://arxiv.org/abs/1506.01497) で微調整して訓練\n",
        "\n",
        "* Faster RCNNについては [Faster-RCNNの仕組みをより深く理解するために](https://medium.com/@whatdhack/a-deeper-look-how-faster-rcnn-works-84081284e1cd) の Media 参照。\n",
        "* [Pytorch 公式チュートリアル文書](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) も参照\n",
        "\n",
        "転移学習 transfer learning と微調整 fine tuning については，種々考え方がある。\n",
        "だがここでは，[PyTorch のチュートリアル](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) に従って，\n",
        "最終層だけ入れ替えて，最終直下層以下の結合係数を固定して考える場合を転移学習と呼ぶことにする。\n",
        "全層を再学習することを，微調整と呼ぶことにする。\n",
        "このチュートリアルが参照にしているのは，Karpathy の スタンフォードでの授業 [cs231n の転移学習のノート](https://cs231n.github.io/transfer-learning/) である。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7byxbK3hj2W"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*1Mj0C4wzi57Z6Z933gb6vA.png\" width=\"66%\"><br/>\n",
        "<div style=\"backgournd-color:cornsilk;width:66%;text-align:left\">    \n",
        "\n",
        "図 Faster-RCNN のブロック図    <!-- Fig 1: Faster-RCNN block diagram.  -->\n",
        "赤紫色のブロックは訓練時のみ活性化<!--The magenta colored blocks are active only during training. -->\n",
        "数値はテンソルサイズ<!--The numbers indicate size of the tensors.-->\n",
        "画像出典: Goswami [A deeper look at how Faster-RCNN works](https://whatdhack.medium.com/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd)\n",
        "<!-- source: Subrata Goswami [A deeper look at how Faster-RCNN works](https://whatdhack.medium.com/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd)-->\n",
        "</div>    \n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKXKS7G8hj2W"
      },
      "source": [
        "<!-- <div class=\"fig\">\n",
        "<img src=\"figures/2020Beery_fig3.svg\" width=\"88%\"><br/>\n",
        "Beery et al. (2020) Fig. 3, `Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection`, arXiv:1912.03538\n",
        "</div> -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycKTURCshj2X"
      },
      "source": [
        "# 1. インストールとインポート\n",
        "<!-- ## Installs and Imports -->\n",
        "\n",
        "## 1.1 下準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhR7jlZxhj2X",
        "outputId": "40006969-ef23-40c5-8d53-6d3bdf1fd38e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting japanize_matplotlib\n",
            "  Downloading japanize-matplotlib-1.1.3.tar.gz (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from japanize_matplotlib) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (1.21.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->japanize_matplotlib) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->japanize_matplotlib) (1.15.0)\n",
            "Building wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.3-py3-none-any.whl size=4120275 sha256=eaf86204540044c7a36b13fa942ba2c965b1b398240218b4193ae9215e85b70c\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/97/6b/e9e0cde099cc40f972b8dd23367308f7705ae06cd6d4714658\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.1.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement albumentataions (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for albumentataions\u001b[0m\n",
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 150582, done.\u001b[K\n",
            "remote: Counting objects: 100% (2958/2958), done.\u001b[K\n",
            "remote: Compressing objects: 100% (386/386), done.\u001b[K\n",
            "remote: Total 150582 (delta 2593), reused 2860 (delta 2561), pack-reused 147624\u001b[K\n",
            "Receiving objects: 100% (150582/150582), 297.34 MiB | 27.85 MiB/s, done.\n",
            "Resolving deltas: 100% (133169/133169), done.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: japanize_matplotlib in /usr/local/lib/python3.7/dist-packages (1.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from japanize_matplotlib) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (1.21.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->japanize_matplotlib) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->japanize_matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import typing\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install japanize_matplotlib\n",
        "    !pip install albumentataions\n",
        "\n",
        "if not isColab:\n",
        "    # 自分のリポジトリからシンボリックリンクで代用\n",
        "    for file in ['engine.py', 'utils.py', 'coco_utils.py', 'transforms.py', 'coco_eval.py']:\n",
        "        if not os.path.exists(file):\n",
        "            _file = os.path.join('../2020pytorch_vision.git/reference/detection/', file)\n",
        "            !ln -s ../2020pytorch_vision.git/references/detection/engine.py .\n",
        "            !ln -s ../2020pytorch_vision.git/references/detection/utils.py .\n",
        "            !ln -s ../2020pytorch_vision.git/references/detection/coco_utils.py .\n",
        "            !ln -s ../2020pytorch_vision.git/references/detection/transforms.py .\n",
        "            !ln -s ../2020pytorch_vision.git/references/detection/coco_eval.py .\n",
        "else:\n",
        "    !pip install pycocotools --quiet\n",
        "    !git clone https://github.com/pytorch/vision.git\n",
        "    !git checkout v0.3.0\n",
        "\n",
        "    # Download TorchVision repo to use some files from references/detection\n",
        "    # os.symlink(src,dst) にした方が良いかも\n",
        "    !cp vision/references/detection/utils.py ./\n",
        "    !cp vision/references/detection/transforms.py ./\n",
        "    !cp vision/references/detection/coco_eval.py ./\n",
        "    !cp vision/references/detection/engine.py ./\n",
        "    !cp vision/references/detection/coco_utils.py ./\n",
        "    \n",
        "    !pip install japanize_matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBLFTuWYhj2Y"
      },
      "source": [
        "## 1.2 ライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI_VHBe6hj2Z"
      },
      "outputs": [],
      "source": [
        "# ライブラリのインポート\n",
        "# python と機械学習のための基本ライブラリ\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import japanize_matplotlib\n",
        "\n",
        "# torchvision ライブラリ\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as torchtrans  \n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "# torchvision.models.detection.retinanet_resnet50_fpn` で RetinaNet を使用してみることもできる。\n",
        "# SSDlite なら `torchvision.models.detection.ssdlite320_mobilenet_v3_large`\n",
        "# SSD は `torchvision.models.detection.ssd300_vgg16` を用いる\n",
        "# これらモデルの詳細については `https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html#instance-seg-output`\n",
        "\n",
        "# ヘルパライブラリをインポート\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "# #画像のデータ拡張 当面は不要\n",
        "# だが `get_transform()` で 用いているため試しに使ってみる\n",
        "# import albumentations as A\n",
        "# from albumentations.pytorch.transforms import ToTensorV2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQP1kmPrhj2a"
      },
      "source": [
        "# 2. データセットの作成\n",
        "\n",
        "## 2.1 自作ライブラリの読み込み，下請け関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageFont\n",
        "from glob import glob\n",
        "\n",
        "if isColab:\n",
        "    !git clone https://github.com/ShinAsakawa/bit.git\n",
        "\n",
        "    !mkdir Noto_JP_fonts\n",
        "    !wget https://noto-website-2.storage.googleapis.com/pkgs/NotoSerifJP.zip\n",
        "    !wget https://noto-website-2.storage.googleapis.com/pkgs/NotoSansJP.zip\n",
        "    !unzip NotoSerifJP.zip -d Noto_JP_fonts\n",
        "    !unzip -o NotoSansJP.zip -d Noto_JP_fonts  # `-o` means overwrite \n",
        "    !mv Noto_JP_fonts bit\n",
        "    !mkdir data\n",
        "    \n",
        "    noto_font_dir = './bit/Noto_JP_fonts'\n",
        "    notofonts_fnames = glob(os.path.join(noto_font_dir,'*otf'))\n",
        "    notofonts = {fname.split('/')[-1].split('.')[0]:{'fname':fname} for fname in notofonts_fnames}\n",
        "    for fontname in notofonts.keys():\n",
        "        notofonts[fontname]['data'] = ImageFont.truetype(notofonts[fontname]['fname'])\n"
      ],
      "metadata": {
        "id": "iUn_UAJQm5GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvEsBcH4hj2a",
        "outputId": "0d7da43a-a1e2-44cc-f38a-5abdf2a5a1a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "bboxes:[[[467, 478, 3850, 622], [588, 1094, 3971, 1242], [863, 1486, 4246, 1618], [570, 1786, 3953, 1920]], [[586, 920, 3969, 1054], [572, 1478, 3955, 1620], [570, 1925, 3953, 2055], [616, 2406, 3999, 2538]]], len(bboxes):2\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from bit import BIT\n",
        "if isColab:\n",
        "    bit = BIT(fontdata=notofonts)\n",
        "else:\n",
        "    bit = BIT()\n",
        "images, bboxes = bit.make_line_bisection_task_images(N=2, n_lines=4)\n",
        "\n",
        "print(f'bboxes:{bboxes}, len(bboxes):{len(bboxes)}')\n",
        "\n",
        "# DETR のサンプルプログラムを借用, bounding boxes の描画時に使用\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "COLORS = COLORS * 100\n",
        "\n",
        "import PIL\n",
        "def plot_pilimg_and_bbox(pil_img:PIL.Image.Image, \n",
        "                         bboxes:list):\n",
        "    \"\"\"bounding box (物体を囲む四角形の境界領域のことを境界領域箱と呼ぶ): bbox\n",
        "    PIL 画像を境界領域と共に表示する関数\"\"\"\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.imshow(pil_img)\n",
        "    ax = plt.gca()\n",
        "    for (xmin, ymin, xmax, ymax), c in zip(bboxes, COLORS):\n",
        "        print(f'xmin:{xmin}, ymin:{ymin}')\n",
        "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                   fill=False, color=c, linewidth=2))    \n",
        "\n",
        "# 訓練画像データ用ディレクトリが存在しなければ作成する\n",
        "train_dirname = './data/2022bit_line_bisection/train'\n",
        "test_dirname = './data/2022bit_line_bisection/test'\n",
        "if not os.path.exists('./data/2022bit_line_bisection'):\n",
        "    os.mkdir('./data/2022bit_line_bisection')\n",
        "for dir_name in [train_dirname, test_dirname]:\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.mkdir(dir_name) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6oeLPtahj2b"
      },
      "source": [
        "## 2.2. 線分二等分線用画像の作成と書き出し"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR_PZrc1hj2b",
        "outputId": "95fcb4b5-f007-4134-886e-88a1f08fb55b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 47.1 s, sys: 2.16 s, total: 49.3 s\n",
            "Wall time: 49.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "lines = [3,4,5]                # 一枚の刺激画像に何本の水平線が含まれるか\n",
        "#lines = [3,4,5,6]               # 一枚の刺激画像に何本の水平線が含まれるか\n",
        "#lines = [3,6]                   # 一枚の刺激画像に何本の水平線が含まれるか\n",
        "train_dups, test_dups = 25, 5  # 各条件ごとに何枚画像を生成するか\n",
        "#train_dups, test_dups = 5, 1    # 各条件ごとに何枚画像を生成するか\n",
        "train_bboxes, test_bboxes = [], []\n",
        "train_imgs, test_imgs = [], []\n",
        "\n",
        "# 訓練データセット，テストデータセットの作成\n",
        "for line in lines:\n",
        "    images, bboxes = bit.make_line_bisection_task_images(N=train_dups, n_lines=line)\n",
        "    train_imgs += images\n",
        "    train_bboxes += bboxes\n",
        "    images, bboxes = bit.make_line_bisection_task_images(N=test_dups, n_lines=line)\n",
        "    test_imgs += images\n",
        "    test_bboxes += bboxes\n",
        "\n",
        "# 訓練データセットの書き出し\n",
        "for i, img in enumerate(train_imgs):\n",
        "    stim_fname = f'{i:04d}.png'\n",
        "    stim_fname = os.path.join(train_dirname, stim_fname)\n",
        "    with open(stim_fname, 'wb') as fp:\n",
        "        img.save(fp,format='png')\n",
        "\n",
        "with open(os.path.join(train_dirname,'bboxes.txt'),'w') as fp:\n",
        "    for bbox in train_bboxes:\n",
        "        #for x in bbox:\n",
        "        fp.write(str(bbox)+'\\n')\n",
        "        #fp.write('\\n')\n",
        "            \n",
        "# 検証データセットの書き出し\n",
        "for i, img in enumerate(test_imgs):\n",
        "    stim_fname = f'{i:04d}.png'\n",
        "    stim_fname = os.path.join(test_dirname, stim_fname)\n",
        "    with open(stim_fname, 'wb') as fp:\n",
        "        img.save(fp,format='png')\n",
        "\n",
        "with open(os.path.join(test_dirname,'bboxes.txt'),'w') as fp:\n",
        "    for bbox in test_bboxes:\n",
        "        fp.write(str(bbox)+'\\n')\n",
        "        #for x in bbox:\n",
        "        #    fp.write(str(x))\n",
        "        #fp.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJydrWk8hj2b"
      },
      "source": [
        "## 2.3 線分二等分線課題作成用データ作成クラスの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "mCMUyzUGhj2b"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import glob\n",
        "from torchvision.transforms import functional as F\n",
        "import albumentations as A\n",
        "#from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "def get_transform():\n",
        "    \n",
        "    return A.Compose([\n",
        "        # ToTensorV2 converts image to pytorch tensor without div by 255\n",
        "        ToTensorV2(p=1.0) \n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "\n",
        "class BIT_LineBisection(torch.utils.data.Dataset):\n",
        "    \"\"\"留意事項\n",
        "    1. データセットはタプルを返す。1 つ目の要素は画像の形状，2 つ目の要素は辞書である。\n",
        "    2. 画像はデータセット定義時に指定したサイズでカラーモードは RGB\n",
        "    3. 画像には 4 つのバウンディングボックスがあり，これはボックス内の 4 つのリストとラベルの長さから明らかである。\n",
        "    \"\"\"\n",
        "    def __init__(self, dirname='data/2022bit_line_bisection/train',\n",
        "                 transforms=get_transform):\n",
        "        \n",
        "        self.transforms = transforms\n",
        "        self.dirname = dirname\n",
        "        self.data_fnames = sorted(glob.glob(os.path.join(dirname,'*.png')))\n",
        "        self.n_data = len(self.data_fnames)\n",
        "\n",
        "        # 武藤先生から送信された画像は 4662 X 3289 なので，この縮尺因子で画像も境界ボックスも規格化する\n",
        "        muto_width = 4662\n",
        "        muto_height = 3289\n",
        "        self.height = 224\n",
        "        self.width = 224\n",
        "        self.height_f = 224 / muto_height\n",
        "        self.width_f =  224 / muto_width\n",
        "        #self.height_f *= int(muto_height / muto_width)\n",
        "        \n",
        "        # 境界領域ボックス (左,上,右,下) 4 点からなるデータを `bboxes.txt` から読み込む\n",
        "        bboxes_fname = os.path.join(self.dirname, 'bboxes.txt')\n",
        "        with open(bboxes_fname, 'r') as fp:\n",
        "            X = fp.readlines()\n",
        "        self.bboxes = {}\n",
        "        for i, line in enumerate(X):\n",
        "            digs = np.array([int(d) for d in line.strip().replace('[','').replace(']','').split(',')])\n",
        "            self.bboxes[i] = np.reshape(digs,((-1,4)))\n",
        "            \n",
        "            for box in self.bboxes[i]:\n",
        "                box[0] *= self.width_f\n",
        "                box[1] *= self.height_f\n",
        "                box[2] *= self.width_f\n",
        "                box[3] *= self.height_f\n",
        "            \n",
        "    def __getitem__(self, index):\n",
        "        #img = plt.imread(self.data_fnames[index])\n",
        "        #img = torch.Tensor(img).permute(2,0,1)\n",
        "        \n",
        "        # reading the images and converting them to correct size and color    \n",
        "        img = cv2.imread(self.data_fnames[index])\n",
        "        #img = cv2.imread(image_path)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
        "        # diving by 255\n",
        "        img_res /= 255.0\n",
        "        img = img_res\n",
        "        img = torch.Tensor(img).permute(2,0,1)\n",
        "        \n",
        "        # convert boxes into a torch.Tensor\n",
        "        bboxes = torch.as_tensor(self.bboxes[index], dtype=torch.float32)\n",
        "        \n",
        "        # getting the areas of the boxes\n",
        "        area = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
        "        \n",
        "         # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((bboxes.shape[0],), dtype=torch.int64)\n",
        "        \n",
        "        labels = torch.as_tensor([bit.symbols.index('<line>') for _ in range(train_dataset.bboxes[0].shape[0])],\n",
        "                                 dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = bboxes\n",
        "        target[\"labels\"] = labels.unsqueeze(0)\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        # image_id\n",
        "        image_id = torch.tensor([index])\n",
        "        target[\"image_id\"] = image_id        \n",
        "        \n",
        "        #transformed = self.transforms(image = img,\n",
        "        #                              bboxes = target['boxes'],\n",
        "        #                              labels = labels)\n",
        "        #img = transformed['image']\n",
        "        \n",
        "        return img, target\n",
        "        #return img, self.bboxes[index]\n",
        "        #return self.data_fnames[index], self.bboxes[index]\n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.n_data\n",
        "            \n",
        "train_dataset = BIT_LineBisection()\n",
        "test_dataset = BIT_LineBisection(dirname='data/2022bit_line_bisection/test')\n",
        "\n",
        "#print(train_dataset.__len__())\n",
        "img, target = train_dataset.__getitem__(9)\n",
        "#plot_pilimg_and_bbox(img.permute(1,2,0).detach().numpy(), target['boxes'])\n",
        "##print(bboxes)\n",
        "##plt.imshow(img)\n",
        "##print(type(img), img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCFPLeV0hj2c",
        "outputId": "05b3957b-ef23-4997-fde3-6501eecd58aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0 0.25882354\n"
          ]
        }
      ],
      "source": [
        "img = plt.imread(train_dataset.data_fnames[0])\n",
        "#type(img)\n",
        "#img.shape\n",
        "print(img.max(), img.min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAvExVUshj2c",
        "outputId": "6017dc94-3397-4705-dd59-4d435f1471a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224]) {'boxes': tensor([[ 25.,  12., 188.,  21.],\n",
            "        [ 20.,  19., 183.,  28.],\n",
            "        [ 34.,  36., 196.,  46.]]), 'labels': tensor([[0, 0, 0]]), 'area': tensor([1467., 1467., 1620.]), 'iscrowd': tensor([0, 0, 0]), 'image_id': tensor([7])}\n"
          ]
        }
      ],
      "source": [
        "img, target = train_dataset[7]\n",
        "print(img.shape, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLg74sn7hj2c"
      },
      "source": [
        "## 2.4. 作成した画像データを視覚化して例示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "qcC9Wxvkhj2c",
        "outputId": "deb7db2c-d386-4d74-91dc-35c1c71a83f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xmin:41.0, ymin:39.0\n",
            "xmin:41.0, ymin:85.0\n",
            "xmin:33.0, ymin:121.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "img, target = test_dataset.__getitem__(0)\n",
        "plot_pilimg_and_bbox(img.permute(1,2,0).detach().numpy(), target['boxes'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXrdIgRMhj2d"
      },
      "source": [
        "## 2.5. PyTorch 用データセットの作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HywIABD7hj2d",
        "outputId": "260643a3-b98b-424d-f3ff-40c742feae2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0, type(imgs[0]):<class 'torch.Tensor'>, imgs[0].size():torch.Size([3, 224, 224]), boxes[1]:tensor([ 29.,  39., 192.,  48.])\n",
            "epoch:1, type(imgs[0]):<class 'torch.Tensor'>, imgs[0].size():torch.Size([3, 224, 224]), boxes[1]:tensor([ 35.,  79., 198.,  88.])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "# 学習・検証用データローダの定義 # define training and validation data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                           batch_size=1,\n",
        "                                           #batch_size=32, \n",
        "                                           shuffle=True, \n",
        "                                           num_workers=0,\n",
        "                                           collate_fn=utils.collate_fn)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                          batch_size=20, \n",
        "                                          shuffle=False, \n",
        "                                          num_workers=0,\n",
        "                                          collate_fn=utils.collate_fn)\n",
        "\n",
        "#print(f'len(train_dataset):{len(train_dataset)}, len(test_dataset):{len(test_dataset)}')\n",
        "#print(bit.symbols)\n",
        "\n",
        "for epoch, (imgs, target) in enumerate(train_loader):\n",
        "    #print(type(target), len(target), target)\n",
        "    bboxes = target[0]['boxes']\n",
        "    print(f'epoch:{epoch}, type(imgs[0]):{type(imgs[0])}, imgs[0].size():{imgs[0].size()}, boxes[1]:{bboxes[1]}')\n",
        "    if epoch >= 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQek4_Rjhj2d",
        "outputId": "0441ca9c-3f2f-4905-f309-c7457414b504"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'area': tensor([1458., 1467., 1467., 1458., 1467.]),\n",
              "  'boxes': tensor([[ 27.,  51., 189.,  60.],\n",
              "          [ 35.,  79., 198.,  88.],\n",
              "          [ 34., 114., 197., 123.],\n",
              "          [ 38., 147., 200., 156.],\n",
              "          [ 25., 172., 188., 181.]]),\n",
              "  'image_id': tensor([50]),\n",
              "  'iscrowd': tensor([0, 0, 0, 0, 0]),\n",
              "  'labels': tensor([[0, 0, 0]])},)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOv6RUUhhj2d"
      },
      "source": [
        "# 3. モデルの定義\n",
        "\n",
        "<!-- モデルを読み込むための関数を定義します。この関数は後で呼び出すことにします -->\n",
        "<!-- We will define a function for loading the model. We will call it later -->\n",
        "\n",
        "## 3.1 訓練済モデルのラベルを日本語で定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBe4G0FHhj2d"
      },
      "outputs": [],
      "source": [
        "# https://github.com/amikelive/coco-labels を参考に日本語訳したもの 全 91 種類\n",
        "CLASSES = [\n",
        "    '背景',                                                       # 0\n",
        "    '人', '自転車', '車', 'バイク', '飛行機',                        # [01-05]\n",
        "    'バス', '電車', 'トラック','ボート', '信号機',                    # [06-10]\n",
        "    '消火栓', '道路標識', '停止サイン', 'パーキングメーター', 'ベンチ',   # [11-15]\n",
        "    '鳥', '猫', '犬', '馬', '羊',                                   # [16-20]\n",
        "    '牛', 'ゾウ', '熊',  'シマウマ', 'キリン',                        # [21-25]\n",
        "    '帽子', 'リュックサック', '傘', '靴', 'メガネ',                    # [26-30]\n",
        "    'ハンドバッグ',  'ネクタイ', 'スーツケース', 'フリスビー', 'スキー',   # [31-35\n",
        "    'スノーボード','スポーツボール','凧', '野球バット', '野球グローブ',    # [36-40]\n",
        "    'スケートボード', 'サーフボード', 'テニスラケット', 'ボトル', '皿',    # [41-45]\n",
        "     'ワイングラス', 'カップ', 'フォーク', 'ナイフ', 'スプーン',          # [46-50]\n",
        "    'ボウル', 'バナナ', 'りんご', 'サンドイッチ', 'オレンジ',            # [51-55]\n",
        "    'ブロッコリー', 'ニンジン', 'ホットドッグ', 'ピザ', 'ドーナツ',      # [56-60]\n",
        "    'ケーキ', '椅子', 'ソファ', '鉢植え', 'ベッド',                   # [51-65]\n",
        "    '鏡', 'ダイニングテーブル', '窓', 'デスク', 'トイレ',              # [66-70]\n",
        "    'ドア', 'テレビ', 'ノートパソコン', 'マウス', 'リモコン',          # [71-75]\n",
        "    'キーボード',  '携帯電話', '電子レンジ', 'コンロ', 'トースター',    # [76-80]\n",
        "    '洗面台', '冷蔵庫', 'ミキサー', '本', '時計',                    # [81-85]\n",
        "    '花瓶', 'ハサミ', 'テディベア', 'ドライヤー', '歯ブラシ']          # [85-90]\n",
        "\n",
        "# DETR のサンプルプログラムを借用\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "COLORS = COLORS * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BX1Rqykhj2e"
      },
      "source": [
        "## 3.2 訓練済モデルの取得関数を定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498,
          "referenced_widgets": [
            "433e0dc2276b4ffcad4ab617d879e36e",
            "9797b4a4f4f945dcaa207ad714eba733",
            "019f3eaa7d174357a16dacfba29b7bc9",
            "0eb56d9c5dc44b199b6b5ba8867e6be0",
            "7f0a475a5b0144359caf0b686eb4a71b",
            "2d93f5633a4843639b401b1908df43b6",
            "c4b148b45e2742c39d5992a5cf690940",
            "e15ebbde55994f1990182b873a75d2a5",
            "e5a62929c82a48259ffda92354b5707e",
            "15d62b8efedf40ea8b062603b57cf6a5",
            "3f71d5b5612d4fd08df0093f2828bb1f"
          ]
        },
        "id": "8-b9uo3Rhj2e",
        "outputId": "29a2da25-dfd6-460d-a9d5-c75f7f445bd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "433e0dc2276b4ffcad4ab617d879e36e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "変換前 model.roi_heads:RoIHeads(\n",
            "  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
            "  (box_head): TwoMLPHead(\n",
            "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (box_predictor): FastRCNNPredictor(\n",
            "    (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
            "    (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
            "  )\n",
            ")\n",
            "変換後 model.roi_heads:RoIHeads(\n",
            "  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
            "  (box_head): TwoMLPHead(\n",
            "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (box_predictor): FastRCNNPredictor(\n",
            "    (cls_score): Linear(in_features=1024, out_features=48, bias=True)\n",
            "    (bbox_pred): Linear(in_features=1024, out_features=192, bias=True)\n",
            "  )\n",
            ")\n",
            "num_classes:48, bit.symbols:['<line>', '★', 'あ', 'い', 'う', 'え', 'お', 'か', 'き', 'く', 'け', 'こ', 'さ', 'し', 'す', 'せ', 'そ', 'た', 'ち', 'つ', 'て', 'と', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ひ', 'ふ', 'へ', 'ほ', 'ま', 'み', 'む', 'め', 'も', 'や', 'ゆ', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'わ', 'を', 'ん']\n"
          ]
        }
      ],
      "source": [
        "def get_object_detection_model(num_classes):\n",
        "\n",
        "    # MS-COCO で事前に学習させたモデルを読み込み\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    #model = torchvision.models.mobilenet_v2(pretrained=True)\n",
        "    \n",
        "    #model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "    #model = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
        "    #retinanet_model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
        "    #print(retinanet_model.head)\n",
        "\n",
        "    #ssdlite_model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(pretrained=True)\n",
        "    #print(ssdlite_model.head.classification_head)\n",
        "    #print(ssdlite_model.head.regression_head)\n",
        "\n",
        "    # 分類器の入力特徴数の取得\n",
        "    #print(model.roi_heads) \n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    print(f'変換前 model.roi_heads:{model.roi_heads}')\n",
        "\n",
        "    # 事前学習済頭部を新しいものに置き換え\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
        "    print(f'変換後 model.roi_heads:{model.roi_heads}')\n",
        "\n",
        "    return model\n",
        "\n",
        "# 自作ヘルパー関数を使ってモデルの宣言\n",
        "num_classes = len(bit.symbols)\n",
        "model = get_object_detection_model(num_classes)\n",
        "print(f'num_classes:{num_classes}, bit.symbols:{bit.symbols}')\n",
        "#print(f'model:{model}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE2BqKxShj2e",
        "outputId": "a2323cb2-1c5a-491e-e17a-150e04b9dd7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RoIHeads(\n",
              "  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "  (box_head): TwoMLPHead(\n",
              "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "  )\n",
              "  (box_predictor): FastRCNNPredictor(\n",
              "    (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
              "    (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "#model = torchvision.models.mobilenet_v2(pretrained=True)    \n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "#[(name, param.size()) for name, param in model.named_parameters()]\n",
        "model.roi_heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS_HgPdLhj2e"
      },
      "source": [
        "<!-- pytorch を使用すると，モデルの読み込みと準備が非常に簡単になることがおわかりいただけると思います。 -->\n",
        "<!-- You can clearly see, how easy it is to load and prepare the model using pytorch -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3dwBCfPhj2e"
      },
      "source": [
        "# 7. 訓練 <!-- # Training-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoxsFZvvhj2e"
      },
      "source": [
        "訓練用のモデルを準備\n",
        "<!-- Let's prepare the model for training -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI6LlCRGhj2e"
      },
      "outputs": [],
      "source": [
        "#[(name, parameter.size()) for name, parameter in model.named_parameters()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYuvDgGUhj2e"
      },
      "outputs": [],
      "source": [
        "# 可能なら GPU 上で学習させる\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "num_classes = len(bit.symbols)\n",
        "\n",
        "# 自作ヘルパー関数を使ってモデルを取得\n",
        "model = get_object_detection_model(num_classes)\n",
        "\n",
        "# モデルを正しいデバイスへ移動\n",
        "model.to(device)\n",
        "\n",
        "# 最適化関数の構成と設定  # construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(params, lr=0.001)\n",
        "\n",
        "# そして 3 エポックごとに学習率を 10 倍ずつ下げる学習率スケジューラ\n",
        "# and a learning rate scheduler which decreases the learning rate by 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6jrpOExhj2f"
      },
      "outputs": [],
      "source": [
        "# 10 エポック分訓練 # training for 10 epochs\n",
        "num_epochs = 10\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'エポック:{epoch}')\n",
        "\n",
        "    model.train()\n",
        "    # 1 エポック分の訓練  # training for one epoch\n",
        "    #train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n",
        "    for images, targets in train_loader:\n",
        "        print(f'epoch:{epoch}, targets:{targets}') #, targets[1]:{targets[1]}')\n",
        "        loss = model(images, targets)\n",
        "        #loss = model(images, (targets,))\n",
        "    \n",
        "    # 学習率の更新  # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    #テストデータセットで評価  # evaluate on the test dataset\n",
        "    evaluate(model, test_loader, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAnbCqvRhj2f"
      },
      "outputs": [],
      "source": [
        "a = np.array([1,2,3])\n",
        "b = np.array([4,5,6])\n",
        "c = np.array([7,8])\n",
        "for a, b, c:\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVEezC6qhj2f"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGmikWC_hj2f"
      },
      "outputs": [],
      "source": [
        "len(bit.symbols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8OQmZ1ahj2f"
      },
      "source": [
        "# 9. モデルの検証 <!-- # Testing our Model -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XfZySswhj2f"
      },
      "outputs": [],
      "source": [
        "# 検証セットから画像を 1 枚選ぶ\n",
        "# pick one image from the test set\n",
        "img, target = dataset_test[np.random.choice(len(dataset))]\n",
        "\n",
        "# モデルを評価モードにする\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])[0]\n",
        "    \n",
        "print('predicted #boxes: ', len(prediction['labels']))\n",
        "print('real #boxes: ', len(target['labels']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4PkeB0zhj2f"
      },
      "source": [
        "うおぉぉぉ たくさんの BOX があります。\n",
        "それらをプロットして，何が予測されたかをチェックしましょう。\n",
        "<!-- Whoa! Thats a lot of bboxes. Lets plot them and check what did it predict -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niBICLuGhj2f"
      },
      "outputs": [],
      "source": [
        "#print('期待される出力')\n",
        "plot_img_bbox(torch_to_pil(img), target, title='グランドトルース')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbgjM_wJhj2f"
      },
      "outputs": [],
      "source": [
        "print('MODEL OUTPUT')\n",
        "plot_img_bbox(torch_to_pil(img), prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoqWuQylhj2f"
      },
      "source": [
        "このモデルは，すべてのリンゴについて多くのバウンディングボックスを予測していることがわかります。\n",
        "これに nms を適用し，最終的な出力を見てみましょう。\n",
        "<!-- You can see that our model predicts a lot of bounding boxes for every apple. Lets apply nms to it and see the final output -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_5vvQhAhj2f"
      },
      "outputs": [],
      "source": [
        "nms_prediction = apply_nms(prediction, iou_thresh=0.2)\n",
        "print('NMS APPLIED MODEL OUTPUT')\n",
        "plot_img_bbox(torch_to_pil(img), nms_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSXqEX0bhj2g"
      },
      "source": [
        "では，テストセットから画像を取り出し，予測してみましょう。\n",
        "<!-- Now lets take an image from the test set and try to predict on it -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3vGeigYhj2g"
      },
      "outputs": [],
      "source": [
        "test_dataset = FruitImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n",
        "\n",
        "# テストセットから画像を 1 枚選ぶ\n",
        "# pick one image from the test set\n",
        "#img, target = test_dataset[10]\n",
        "img, target = test_dataset[np.random.choice(len(test_dataset))]\n",
        "\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])[0]\n",
        "    \n",
        "#print('期待される出力\\n')\n",
        "# print('EXPECTED OUTPUT\\n')\n",
        "plot_img_bbox(torch_to_pil(img), target, title=\"グランドトルース\")\n",
        "\n",
        "#print('モデルの出力\\n')\n",
        "# print('MODEL OUTPUT\\n')\n",
        "nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n",
        "\n",
        "plot_img_bbox(torch_to_pil(img), nms_prediction, title=\"モデル予測\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa9wMO6phj2g"
      },
      "source": [
        "このモデルは，単一物体の画像ではよくできています。\n",
        "<!-- The model does well on single object images.-->\n",
        "\n",
        "モデルはスライスも予測していることがわかりますが，これは失敗を意味します☹️ 。\n",
        "しかし心配しないでください。\n",
        "これは単なる基本的なモデルであり，我々がそれを改善できるいくつかのアイデアがあります:\n",
        "<!-- You can see that our model predicts the slices too and that means a failure ☹️ . But fear not, this is just a base line model here are some ideas we can improve it -  -->\n",
        "\n",
        "1. より良いモデルを使用する。\n",
        "現在使用しているモデルのバックボーンである `resnet50` を変更し，微調整を行うことが可能です。\n",
        "2. 画像の大きさ，最適化関数，学習速度のスケジュールなどの学習設定を変更することができる。\n",
        "3. データ拡張を追加することができる。\n",
        "ここでは，データ拡張関数の豊富なライブラリを持つ Albumentations ライブラリを使用しました。\n",
        "自由に探索し，試してみてください。\n",
        "\n",
        "<!-- 1. Use a better model. \n",
        "   We have the option of changing the backbone of our model which at present is `resnet 50` and the fine tune it.\n",
        "2. We can change the training configurations like size of the images, optimizers and learning rate schedule.\n",
        "3. We can add more augmentations.\n",
        "   We have used the Albumentations library which has an extensive library of data augmentation functions. Feel free to explore and try them out.  -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBHJjy9Jhj2g"
      },
      "source": [
        "# 10. 結果の保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSE8MzSXhj2g"
      },
      "outputs": [],
      "source": [
        "torch.save({'model': model.state_dict(),\n",
        "            'optim': optimizer.state_dict()\n",
        "           }, '2022_0318fine_tune_fruit.cpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqDGsJOHhj2g"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6vGGUmVhj2g"
      },
      "source": [
        "`torchvision.transfomrs.functional` あるいは `F` では，3 つ `*.to_*()` 関数が定義されている。\n",
        "すなわち以下の 3 つである:\n",
        "\n",
        "* `F.to_grayscale(img, num_output_channels=1))->PIL_Image`: img:PIL_Image をグレイスケールに変換\n",
        "* `F.to_pil_image(pic, mode=None)->PIL_Image`: pic:[tensor|ndarray] を PIL_Image へ変換\n",
        "* `F.to_tensor(pic)->torch.Tensor`: pic:[PIL_Image|np.ndarray]  を tensor へ変換\n",
        "* `F.pil_to_tensor(pic:PIL_Image)->torch.Tensor`: \n",
        "逆に torch.Tensor を変換する関数は無いのかしら?\n",
        "\n",
        "* `F.pil_to_tensor(pic:PIL_Image)->torch.Tensor`\n",
        "`F.convert_image_dtype(img:torch.Tensor, dtype:torch.dtype=torch.float32)->torch.Tensor`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJZiRWRYhj2g"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "def plot_pilimg_and_bbox(pil_img:PIL.Image.Image, \n",
        "                         bboxes:list):\n",
        "    \"\"\"bounding box (物体を囲む四角形の境界領域のことを境界領域箱と呼ぶ): bbox\n",
        "    PIL 画像を境界領域と共に表示する関数\"\"\"\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.imshow(pil_img)\n",
        "    ax = plt.gca()\n",
        "    for (xmin, ymin, xmax, ymax), c in zip(bboxes, COLORS):\n",
        "        print(f'xmin:{xmin}, ymin:{ymin}')\n",
        "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                   fill=False, color=c, linewidth=2))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTQUyRXAhj2g"
      },
      "outputs": [],
      "source": [
        "print(dir(F))\n",
        "#help(F.pil_to_tensor)\n",
        "#help(F.torch)\n",
        "help(F.pil_to_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Fz34zNzHhj2g"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
        "\n",
        "def torch_imgs_show(imgs):\n",
        "    \"\"\"Torch tensor に変換済の画像を表示する\"\"\"\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        if not isinstance(img, PIL.Image.Image):\n",
        "            #img = img.detach()\n",
        "            img = F.to_pil_image(img.detach())\n",
        "        axs[0, i].imshow(np.asarray(img))\n",
        "        #axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])        \n",
        "\n",
        "#torch_imgs_show(images)        \n",
        "#[torch.Tensor(np.array(img)).permute(1,2,0) for img in images]\n",
        "torch_imgs_show(images[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWPySLwphj2g"
      },
      "outputs": [],
      "source": [
        "#help(F.to_pil_image)\n",
        "_images = [F.to_pil_image(np.array(img)) for img in images]\n",
        "#type(images[0])\n",
        "type(_images[0])\n",
        "#help(F.to_grayscale)\n",
        "display(images[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEg8KmP2hj2h"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "#help(glob.glob)\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nXKldqThj2h"
      },
      "outputs": [],
      "source": [
        "X = np.array([[1,2,3],[4,5,6]])\n",
        "np.savetxt('xx.txt',X, fmt='%.d')\n",
        "!cat xx.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mInnsjtBhj2h"
      },
      "outputs": [],
      "source": [
        "#help(np.ndarray.reshape)\n",
        "help(np.savetxt)\n",
        "#np.fromfile('data/2022bit_line_bisection/test/test_bboxes.txt',dtype=int).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqV0yJrZhj2h"
      },
      "outputs": [],
      "source": [
        "#print(line_bisection[0][1]['boxes']) # .squeeze(0).size()[0])\n",
        "#print(line_bisection[0][1]['labels'])\n",
        "\n",
        "N = 95\n",
        "_, target = line_bisection.__getitem__(N)\n",
        "print(f'target[\"boxes\"].squeeze(0).size():{target[\"boxes\"].squeeze(0).size()}')\n",
        "print(f'len(target[\"labels\"][1]:{len(target[\"labels\"][1])}')\n",
        "print(f'len(target[\"boxes\"].squeeze(0):{len(target[\"boxes\"].squeeze(0))}')\n",
        "print(f'line_bisection[N][1]:{line_bisection[N][1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76krEiplhj2h"
      },
      "outputs": [],
      "source": [
        "#line_bisection.__getitem__(95)[1]\n",
        "[_ for _ in target['labels']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTc8GgpZhj2h"
      },
      "outputs": [],
      "source": [
        "N = line_bisection.__len__()\n",
        "n_imgs = list(np.random.permutation(N))[:2]\n",
        "for x in n_imgs:\n",
        "    img, target = line_bisection.__getitem__(x)\n",
        "    colors = ['cyan' for _ in target['labels']] # [1]]\n",
        "    print(f'x:{x}, len(colors){len(colors)}', \n",
        "          f'colors:{colors}')\n",
        "    #print(f'targets[\"boxes\"]:{target[\"boxes\"]}')\n",
        "    \n",
        "    result = draw_bounding_boxes(img, target['boxes'].squeeze(0), colors=colors, width=15)\n",
        "    torch_imgs_show(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt97Vr8ohj2h"
      },
      "outputs": [],
      "source": [
        "line_bisection[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cyAep0Khj2h"
      },
      "source": [
        "`class FasterRCNN(torchvision.models.detection.generalized_rcnn.GeneralizedRCNN)`\n",
        "\n",
        "```python\n",
        "class FasterRCNN(backbone, \n",
        "    num_classes=None, min_size=800, max_size=1333, \n",
        "    image_mean=None, image_std=None, \n",
        "    rpn_anchor_generator=None, rpn_head=None, \n",
        "    rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000, \n",
        "    rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000, \n",
        "    rpn_nms_thresh=0.7, rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3, \n",
        "    rpn_batch_size_per_image=256, rpn_positive_fraction=0.5, rpn_score_thresh=0.0, \n",
        "    box_roi_pool=None, box_head=None, box_predictor=None, box_score_thresh=0.05, \n",
        "    box_nms_thresh=0.5, box_detections_per_img=100, \n",
        "    box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5, \n",
        "    box_batch_size_per_image=512, box_positive_fraction=0.25, bbox_reg_weights=None)\n",
        "```\n",
        "\n",
        "### Fastr R-CNN の実装 \n",
        "<!-- Implements Faster R-CNN.-->\n",
        "\n",
        "モデルへの入力はテンソルのリストであり，それぞれ [C, H, W] の形状で，各画像に対して1つずつ，0-1 の範囲であることが望ましいとされる。\n",
        "異なる画像は異なるサイズを持つことができる。\n",
        "<!-- The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. \n",
        "Different images can have different sizes.-->\n",
        "\n",
        "モデルの動作は，学習モードか評価モードかによって変化する。\n",
        "<!-- The behavior of the model changes depending if it is in training or evaluation mode. -->\n",
        "\n",
        "学習時，モデルは入力テンソルと，それを含むターゲット (辞書のリスト) の両方が期待される。\n",
        "<!-- During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing: -->\n",
        "\n",
        "- boxes (``FloatTensor[N, 4]``): ``[x1, y1, x2, y2]`` 形式のグランドトルース判定用ボックスで ``0 <= x1 < x2 <= W``, ``0 <= y1 < y2 <= H`` である\n",
        "- labels (``Int64Tensor[N]``):  各グラウンドトゥルースボックスに対応するクラスラベル。\n",
        "\n",
        "<!-- - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
        "- labels (Int64Tensor[N]): the class label for each ground-truth box -->\n",
        "\n",
        "このモデルは学習時には，RPN と R-CNN の両方の分類と回帰の損失を含む Dict[Tensor] を返す。\n",
        "<!-- The model returns a Dict[Tensor] during training, containing the classification and regression losses for both the RPN and the R-CNN.-->\n",
        "\n",
        "推論時には，モデルは入力テンソルだけを必要とし，処理後の予測値を List[Dict[Tensor]] として，各入力画像に対して 1 つずつ返す。\n",
        "Dict のフィールドは以下の通り:\n",
        "<!-- During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image.  \n",
        "The fields of the Dict are as follows:-->\n",
        "\n",
        "- boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
        "- labels (``Int64Tensor[N]``): the predicted labels for each image\n",
        "- scores (``Tensor[N]``): the scores or each prediction\n",
        "\n",
        "<!-- - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
        "- labels (Int64Tensor[N]): the predicted labels for each image\n",
        "- scores (Tensor[N]): the scores or each prediction-->\n",
        "\n",
        "#### 引数 Args:\n",
        "\n",
        "* `backbone (nn.Module)`: モデルの特徴を計算するために使用されるネットワーク\n",
        "バックボーンには `out_channels` 属性が必要で，これは各特徴地図が持つ出力チャネルの数を示す (全ての特徴地図で同じであるべき)。\n",
        "バックボーンは単一の Tensor または OrderedDict[Tensor] を返す必要がある。\n",
        "* `num_classes (int)`: モデルの出力クラスの数 (背景も含む)。\n",
        "box_predictor が指定された場合 `num_classes` は None となる。\n",
        "* `min_size(int)`:  バックボーンに渡す前に再スケーリングされる画像の最小サイズ．\n",
        "* `max_size (int)`: バックボーンに渡す前に再スケーリングされる画像の最大サイズ．\n",
        "* `image_mean (Tuple[float, float, float])`: 入力画像の正規化に利用される平均値．\n",
        "一般的にはバックボーンを学習させたデータセットの平均値\n",
        "* `image_std (Tuple[float, float, float])`: 入力の正規化に利用される標準偏差の値．\n",
        "一般的にはバックボーンを学習させたデータセットの標準偏差を用いる。\n",
        "* `rpn_anchor_generator (AnchorGenerator)`: 特徴地図の集合のアンカーを生成するモジュール\n",
        "* `rpn_head (nn.Module)`: RPN から objectness と regression deltas を計算するモジュール\n",
        "* `rpn_pre_nms_top_n_train (int)`: 学習時に NMS を適用する前に保持すべきプロポーザルの数\n",
        "* `rpn_pre_nms_top_n_test (int)` : テスト時に NMS を適用する前に保持するプロポーザルの数\n",
        "* `rpn_post_nms_top_n_train (int)`: 訓練中に NMS を適用した後に保持するプロポーザルの数\n",
        "* `rpn_post_nms_top_n_train (int)`:  訓練中に NMS を適用した後に保持するプロポジションの数\n",
        "* `rpn_post_nms_top_n_test (int)`:  テスト時に NMS を適用した後に保持するプロポーザルの数\n",
        "* `rpn_post_nms_top_n_test (int)`:  テスト時にNMSを適用した後に保持するプロポジションの数\n",
        "* `rpn_nms_thresh (float)`:  RPN プロポーザルの後処理に使用する NMS の閾値\n",
        "* `rpn_fg_iou_thresh (float)`: RPN の学習時にアンカーと GT ボックスの間の IoU を正と見なすための最小値\n",
        "* `rpn_bg_iou_thresh (float)`:  アンカーと GT ボックスの間の IoU の最大値 (RPN の学習時に負とみなすことができる) 。\n",
        "* `rpn_batch_size_per_image (int)`:  RPN の学習時に損失を計算するためにサンプリングされるアンカーの数．\n",
        "* `rpn_positive_fraction (float)`:  RPN の訓練中にミニバッチに含まれる正事例アンカーの割合。\n",
        "* `rpn_score_thresh (float)`: 推論時に `rpn_score_thresh` より大きな分類得点を持つプロポーザルのみを返す。\n",
        "* `box_roi_pool (MultiScaleRoIAlign)`: バウンディングボックスで示される位置の特徴地図を切り出し，サイズを変更するモジュール\n",
        "* `box_head (nn.Module)` : 切り出した特徴地図を入力とするモジュール \n",
        "* `box_predictor (nn.Module)`:  box_head の出力を受けて，分類ロジットとボックス回帰デルタを返すモジュール \n",
        "* `box_score_thresh (nn.Module)`:  分類ロジットとボックス回帰デルタを返すモジュール\n",
        "* `box_score_thresh (float)`: 推論時 box_score_thresh より大きい分類得点を持つプロポーザルのみを返す。\n",
        "* `box_nms_thresh (float)`: 予測ヘッドの NMS 閾値。推論時に使用\n",
        "* `box_detections_per_img (int)`:  全てのクラスについて，画像あたりの最大検出数。\n",
        "* `box_fg_iou_thresh (float)`:  提案 と GT ボックスの間の最小 IoU  (\n",
        "分類ヘッドの訓練中に正事例とみなすことができる)．\n",
        "* `box_bg_iou_thresh (float)`:  プロポーザルと GT ボックスの間の最大 IoU (分類ヘッドの訓練中に負事例とみなすことができる)。\n",
        "* `box_batch_size_per_image (int)`: 分類ヘッドの訓練中にサンプリングされるプロポーザルの数。\n",
        "* `box_positive_fraction (float)`: 分類ヘッドの訓練時にミニバッチに含まれる正のプロポーザルの割合。\n",
        "* `bbox_reg_weights (Tuple[float, float, float, float])`: バウンディングボックスの符号化/復号化のための重み付け\n",
        "\n",
        "<!-- * `backbone (nn.Module)`: the network used to compute the features for the model. \n",
        "It should contain a out_channels attribute, which indicates the number of output channels that each feature map has (and it should be the same for all feature maps).\n",
        "The backbone should return a single Tensor or and OrderedDict[Tensor].\n",
        "* `num_classes (int)`: number of output classes of the model (including the background).\n",
        "If box_predictor is specified, num_classes should be None.\n",
        "* `min_size (int)`: minimum size of the image to be rescaled before feeding it to the backbone\n",
        "* `max_size (int)`: maximum size of the image to be rescaled before feeding it to the backbone\n",
        "* `image_mean (Tuple[float, float, float])`: mean values used for input normalization.\n",
        "They are generally the mean values of the dataset on which the backbone has been trained on\n",
        "* `image_std (Tuple[float, float, float])`: std values used for input normalization.\n",
        "They are generally the std values of the dataset on which the backbone has been trained on\n",
        "* rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature maps.\n",
        "* rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN\n",
        "* rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training\n",
        "* rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing\n",
        "* rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training\n",
        "* rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing\n",
        "* rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n",
        "* rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be considered as positive during training of the RPN.\n",
        "* rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be considered as negative during training of the RPN.\n",
        "* rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN for computing the loss\n",
        "* rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training of the RPN\n",
        "* rpn_score_thresh (float): during inference, only return proposals with a classification score greater than rpn_score_thresh \n",
        "* box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in the locations indicated by the bounding boxes\n",
        "* box_head (nn.Module): module that takes the cropped feature maps as input \n",
        "* box_predictor (nn.Module): module that takes the output of box_head and returns the classification logits and box regression deltas.\n",
        "* box_score_thresh (float): during inference, only return proposals with a classification score greater than box_score_thresh\n",
        "* box_nms_thresh (float): NMS threshold for the prediction head. Used during inference\n",
        "* box_detections_per_img (int): maximum number of detections per image, for all classes.\n",
        "* box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be considered as positive during training of the classification head\n",
        "* box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be considered as negative during training of the classification head\n",
        "* box_batch_size_per_image (int): number of proposals that are sampled during training of the classification head\n",
        "* box_positive_fraction (float): proportion of positive proposals in a mini-batch during training of the classification head\n",
        "* bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the bounding boxes\n",
        "-->\n",
        "\n",
        "#### コード例 <!--Example::-->\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# 分類用に事前に学習させたモデルをロードし、特徴量のみを返す\n",
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "\n",
        "# FasterRCNN はバックボーンの出力チャンネル数を知る必要がある\n",
        "# mobilenet_v2 の場合は 1280 なので，ここに追加する必要がある\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "# RPN が空間位置ごとに 5×3 のアンカーを生成するようにする\n",
        "# 各特徴地図は異なるサイズとアスペクト比を持つ可能性があるため `Tuple[Tuple[int]]` を用意した\n",
        "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# 関心領域の切り出しに使用する特徴地図と，切り出し後のサイズを定義する\n",
        "# バックボーンが Tensor を返す場合 `featuremap_names` は `['0’]` であることが期待される。\n",
        "# より一般的にはバックボーンは `OrderedDict[Tensor]` を返すべきで，`featmap_names` で，どの特徴地図を使うかを選択することができる。\n",
        "# let's define what are the feature maps that we will use to perform the region of interest cropping, as well as the size of the crop after rescaling. \n",
        "# If your backbone returns a Tensor, featmap_names is expected to be ['0']. \n",
        "# More generally, the backbone should return an OrderedDict[Tensor], and in featmap_names you can choose which feature maps to use.\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "\n",
        "# put the pieces together inside a FasterRCNN model\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=2,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "hKNJbw3whj2i"
      },
      "source": [
        "# 3. 視覚化\n",
        "<!-- # Visualization -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q2u7oDthj2j"
      },
      "outputs": [],
      "source": [
        "# 画像中のバウンディングボックスを可視化する関数 # Function to visualize bounding boxes in the image\n",
        "\n",
        "def plot_img_bbox(img, target, title=None):\n",
        "    # 画像と bbox を描画     # plot the image and bboxes\n",
        "    # バウンディングボックスは以下のように定義されます: x-min y-min 幅 高さ\n",
        "    # Bounding boxes are defined as follows: x-min y-min width height\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    fig.set_size_inches(7,7)\n",
        "    ax.imshow(img)\n",
        "    print(target)\n",
        "    \n",
        "    for box in target['boxes']:\n",
        "        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
        "        rect = patches.Rectangle((x, y),\n",
        "                                 width, height,\n",
        "                                 linewidth = 4,\n",
        "                                 edgecolor = 'red',\n",
        "                                 facecolor = 'none')\n",
        "\n",
        "        # 画像上にバウンディングボックスを描画 # Draw the bounding box on top of the image\n",
        "        ax.add_patch(rect)\n",
        "        \n",
        "    if title != None:\n",
        "        ax.set_title(title)\n",
        "    plt.show()\n",
        "    \n",
        "# bbox を使って画像を描画。インデックスの変更はご自由に\n",
        "# plotting the image with bboxes. Feel free to change the index\n",
        "#N = np.random.choice(len(dataset))\n",
        "#img, target = dataset[N]\n",
        "#img, target = dataset[25]\n",
        "#img, target = line_bisection.__getitem__(N)\n",
        "#print(img.shape, type(img), target.keys(), type(target))\n",
        "#plot_img_bbox(img[0], target,title=str(N))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tux8tXelhj2j"
      },
      "source": [
        "bbox が正しく配置されているので、今までうまくいっていたことがわかります。\n",
        "<!-- You can see that we are doing great till now, as the bbox is correctly placed. -->\n",
        "\n",
        "一つ注意すべきは，このデータセットでは，完全なリンゴだけを「リンゴ」と予測し，半分にカットされたリンゴは予測しないことを求めている点です。\n",
        "これは克服すべき課題です。\n",
        "<!-- One thing to note is that, the dataset wants us to predict only the full apple as \"apple\" but not the half cut one. \n",
        "This will be a challenge to overcome. -->"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "433e0dc2276b4ffcad4ab617d879e36e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9797b4a4f4f945dcaa207ad714eba733",
              "IPY_MODEL_019f3eaa7d174357a16dacfba29b7bc9",
              "IPY_MODEL_0eb56d9c5dc44b199b6b5ba8867e6be0"
            ],
            "layout": "IPY_MODEL_7f0a475a5b0144359caf0b686eb4a71b"
          }
        },
        "9797b4a4f4f945dcaa207ad714eba733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d93f5633a4843639b401b1908df43b6",
            "placeholder": "​",
            "style": "IPY_MODEL_c4b148b45e2742c39d5992a5cf690940",
            "value": "100%"
          }
        },
        "019f3eaa7d174357a16dacfba29b7bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e15ebbde55994f1990182b873a75d2a5",
            "max": 167502836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5a62929c82a48259ffda92354b5707e",
            "value": 167502836
          }
        },
        "0eb56d9c5dc44b199b6b5ba8867e6be0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15d62b8efedf40ea8b062603b57cf6a5",
            "placeholder": "​",
            "style": "IPY_MODEL_3f71d5b5612d4fd08df0093f2828bb1f",
            "value": " 160M/160M [00:01&lt;00:00, 96.7MB/s]"
          }
        },
        "7f0a475a5b0144359caf0b686eb4a71b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d93f5633a4843639b401b1908df43b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4b148b45e2742c39d5992a5cf690940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e15ebbde55994f1990182b873a75d2a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5a62929c82a48259ffda92354b5707e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15d62b8efedf40ea8b062603b57cf6a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f71d5b5612d4fd08df0093f2828bb1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "colab": {
      "name": "2022_0319bit_faster-rcnn_fine_tuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}