{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/bit/blob/main/notebooks/2022_0611bit_faster_rcnn_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBJdUyBhb1J_"
      },
      "source": [
        "---\n",
        "- date: 2022_0604 original file written at 2022_0319\n",
        "- filename: 2022_0604bit_faster-rcnn_fine_tuning.ipynb\n",
        "- ref source: https://www.kaggle.com/yerramvarun/fine-tuning-faster-rcnn-using-pytorch/notebook\n",
        "---\n",
        "\n",
        "**注**: workers=0 で動作するので時間がかかる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3Y396uYb1KB"
      },
      "source": [
        "# faster-rcnn 転移学習または微調整を用いた BIT 線分 2 等分課題\n",
        "\n",
        "[BIT] 図版を [Faster RCNN](https://arxiv.org/abs/1506.01497) で微調整して訓練\n",
        "\n",
        "* Faster RCNNについては [Faster-RCNNの仕組みをより深く理解するために](https://medium.com/@whatdhack/a-deeper-look-how-faster-rcnn-works-84081284e1cd) の Media 参照。\n",
        "* [Pytorch 公式チュートリアル文書](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) も参照\n",
        "\n",
        "転移学習 transfer learning と微調整 fine tuning については，種々考え方がある。\n",
        "だがここでは，[PyTorch のチュートリアル](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) に従って，\n",
        "最終層だけ入れ替えて，最終直下層以下の結合係数を固定して考える場合を転移学習と呼ぶことにする。\n",
        "全層を再学習することを，微調整と呼ぶことにする。\n",
        "このチュートリアルが参照にしているのは，Karpathy の スタンフォードでの授業 [cs231n の転移学習のノート](https://cs231n.github.io/transfer-learning/) である。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy3XhsTjb1KC"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://github.com/ShinAsakawa/bit/blob/main/notebooks/figures/R_CNN.png?raw=1\" width=\"66%\"><br/>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*1Mj0C4wzi57Z6Z933gb6vA.png\" width=\"66%\"><br/>\n",
        "<div style=\"backgournd-color:cornsilk;width:66%;text-align:left\">\n",
        "\n",
        "図 Faster-RCNN のブロック図 <!-- Fig 1: Faster-RCNN block diagram.  -->\n",
        "赤紫色のブロックは訓練時のみ活性化<!--The magenta colored blocks are active only during training. -->\n",
        "数値はテンソルサイズ<!--The numbers indicate size of the tensors.-->\n",
        "画像出典: Goswami [A deeper look at how Faster-RCNN works](https://whatdhack.medium.com/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd)\n",
        "<!-- source: Subrata Goswami [A deeper look at how Faster-RCNN works](https://whatdhack.medium.com/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd)-->\n",
        "</div>  </center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj2GcaTib1KD"
      },
      "source": [
        "<!-- <div class=\"fig\">\n",
        "<img src=\"figures/2020Beery_fig3.svg\" width=\"88%\"><br/>\n",
        "Beery et al. (2020) Fig. 3, `Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection`, arXiv:1912.03538\n",
        "</div> -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwfe8g60b1KD"
      },
      "source": [
        "# 1. インストールとインポート\n",
        "<!-- ## Installs and Imports -->\n",
        "\n",
        "## 1.1 下準備\n",
        "\n",
        "必要なライブラリのインストールなど"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2022_0605 現在，下記の再インストールが必要のようだ\n",
        "# また，訓練時に halt する\n",
        "!pip install --upgrade albumentations\n",
        "!pip uninstall opencv-python-headless==4.5.5.62\n",
        "!pip install opencv-python-headless==4.5.2.52"
      ],
      "metadata": {
        "id": "p5S5HRKH9DY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9e70fc-2d4c-432e-b679-16461c92cbc2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.21.6)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 48.3 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (4.2.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
            "Installing collected packages: opencv-python-headless, qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.1.0 opencv-python-headless-4.6.0.66 qudida-0.0.4\n",
            "Found existing installation: opencv-python-headless 4.6.0.66\n",
            "Uninstalling opencv-python-headless-4.6.0.66:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/*\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless-4.6.0.66.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libavcodec-5896f664.so.58.134.100\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libavformat-8ef5c7db.so.58.76.100\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libavutil-9c768859.so.56.70.100\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libbz2-a273e504.so.1.0.6\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libcrypto-d21001fc.so.1.1\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libgfortran-91cc3cb1.so.3.0.0\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libopenblas-r0-f650aae0.3.3.so\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libpng16-57e5e0a0.so.16.37.0\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libssl-c8c53640.so.1.1\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libswresample-99364a1c.so.3.9.100\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libswscale-e6451464.so.5.9.100\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libvpx-f22f1483.so.7.0.0\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libQtCore-bbdab771.so.4.8.7\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libQtGui-903938cd.so.4.8.7\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libQtTest-1183da5d.so.4.8.7\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libavcodec-3cdd3bd4.so.58.62.100\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libavformat-69a63b50.so.58.35.100\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libavutil-8e8979a8.so.56.36.100\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libbz2-7225278b.so.1.0.3\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libcrypto-a25ff511.so.1.1\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libssl-fdf0b66c.so.1.1\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libswresample-c6b3bbb9.so.3.6.100\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libswscale-2d19f7d1.so.5.6.100\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libvpx-c887ea55.so.6.1.0\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libz-a147dcb0.so.1.2.3\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/cv2.cpython-37m-x86_64-linux-gnu.so\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled opencv-python-headless-4.6.0.66\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opencv-python-headless==4.5.2.52\n",
            "  Downloading opencv_python_headless-4.5.2.52-cp37-cp37m-manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.2 MB 93.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless==4.5.2.52) (1.21.6)\n",
            "Installing collected packages: opencv-python-headless\n",
            "Successfully installed opencv-python-headless-4.5.2.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "KMPZyEqqb1KE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import typing\n",
        "import cv2\n",
        "import glob\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "\n",
        "if isColab:\n",
        "    from PIL import ImageFont\n",
        "    from glob import glob\n",
        "\n",
        "    !pip install pycocotools --quiet\n",
        "    !git clone https://github.com/pytorch/vision.git\n",
        "    !git checkout v0.3.0\n",
        "\n",
        "    # Download TorchVision repo to use some files from references/detection\n",
        "    # os.symlink(src,dst) にした方が良いかも\n",
        "    !cp vision/references/detection/utils.py ./\n",
        "    !cp vision/references/detection/transforms.py ./\n",
        "    !cp vision/references/detection/coco_eval.py ./\n",
        "    !cp vision/references/detection/engine.py ./\n",
        "    !cp vision/references/detection/coco_utils.py ./\n",
        "    \n",
        "    !pip install japanize_matplotlib\n",
        "    #!pip install albumentataions  # 2022_0604 一時的に中断 colab でエラー発生のため\n",
        "    \n",
        "    # 自作ライブラリ\n",
        "    !git clone https://github.com/ShinAsakawa/bit.git\n",
        "\n",
        "    # Noto fonts のダウンローAドとインストール\n",
        "    !mkdir Noto_JP_fonts\n",
        "    !wget https://noto-website-2.storage.googleapis.com/pkgs/NotoSerifJP.zip\n",
        "    !wget https://noto-website-2.storage.googleapis.com/pkgs/NotoSansJP.zip\n",
        "    !unzip NotoSerifJP.zip -d Noto_JP_fonts\n",
        "    !unzip -o NotoSansJP.zip -d Noto_JP_fonts  # `-o` means overwrite \n",
        "    !mv Noto_JP_fonts bit\n",
        "    !mkdir data\n",
        "    \n",
        "    noto_font_dir = './bit/Noto_JP_fonts'\n",
        "    notofonts_fnames = glob(os.path.join(noto_font_dir,'*otf'))\n",
        "    notofonts = {fname.split('/')[-1].split('.')[0]:{'fname':fname} for fname in notofonts_fnames}\n",
        "    for fontname in notofonts.keys():\n",
        "        notofonts[fontname]['data'] = ImageFont.truetype(notofonts[fontname]['fname'])\n",
        "else:\n",
        "    # 自分のリポジトリからシンボリックリンクで代用\n",
        "    for file in ['engine.py', 'utils.py', 'coco_utils.py', 'transforms.py', 'coco_eval.py']:\n",
        "        if not os.path.exists(file):\n",
        "            _file = os.path.join('../2020pytorch_vision.git/reference/detection/', file)\n",
        "            !ln -s ../2020pytorch_vision.git/references/detection/engine.py .\n",
        "            !ln -s ../2020pytorch_vision.git/references/detection/utils.py .\n",
        "            !ln -s ../2020pytorch_vision.git/references/detection/coco_utils.py .\n",
        "            !ln -s ../2020pytorch_vision.git/references/detection/transforms.py .\n",
        "            !ln -s ../2020pytorch_vision.git/references/detection/coco_eval.py .\n",
        "            \n",
        "            \n",
        "# https://github.com/amikelive/coco-labels を参考に日本語訳したもの 全 91 種類\n",
        "CLASSES = [\n",
        "    '背景',                                                       # 0\n",
        "    '人', '自転車', '車', 'バイク', '飛行機',                        # [01-05]\n",
        "    'バス', '電車', 'トラック','ボート', '信号機',                    # [06-10]\n",
        "    '消火栓', '道路標識', '停止サイン', 'パーキングメーター', 'ベンチ',   # [11-15]\n",
        "    '鳥', '猫', '犬', '馬', '羊',                                   # [16-20]\n",
        "    '牛', 'ゾウ', '熊',  'シマウマ', 'キリン',                        # [21-25]\n",
        "    '帽子', 'リュックサック', '傘', '靴', 'メガネ',                    # [26-30]\n",
        "    'ハンドバッグ',  'ネクタイ', 'スーツケース', 'フリスビー', 'スキー',   # [31-35\n",
        "    'スノーボード','スポーツボール','凧', '野球バット', '野球グローブ',    # [36-40]\n",
        "    'スケートボード', 'サーフボード', 'テニスラケット', 'ボトル', '皿',    # [41-45]\n",
        "     'ワイングラス', 'カップ', 'フォーク', 'ナイフ', 'スプーン',          # [46-50]\n",
        "    'ボウル', 'バナナ', 'りんご', 'サンドイッチ', 'オレンジ',            # [51-55]\n",
        "    'ブロッコリー', 'ニンジン', 'ホットドッグ', 'ピザ', 'ドーナツ',      # [56-60]\n",
        "    'ケーキ', '椅子', 'ソファ', '鉢植え', 'ベッド',                   # [51-65]\n",
        "    '鏡', 'ダイニングテーブル', '窓', 'デスク', 'トイレ',              # [66-70]\n",
        "    'ドア', 'テレビ', 'ノートパソコン', 'マウス', 'リモコン',          # [71-75]\n",
        "    'キーボード',  '携帯電話', '電子レンジ', 'コンロ', 'トースター',    # [76-80]\n",
        "    '洗面台', '冷蔵庫', 'ミキサー', '本', '時計',                    # [81-85]\n",
        "    '花瓶', 'ハサミ', 'テディベア', 'ドライヤー', '歯ブラシ']          # [85-90]\n",
        "\n",
        "# DETR のサンプルプログラムを借用\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "COLORS = COLORS * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOWNVBjIb1KF"
      },
      "source": [
        "## 1.2 ライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WhUrMEyDb1KG"
      },
      "outputs": [],
      "source": [
        "# ライブラリのインポート\n",
        "# python と機械学習のための基本ライブラリ\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import japanize_matplotlib\n",
        "\n",
        "# torchvision ライブラリ\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as torchtrans  \n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "# torchvision.models.detection.retinanet_resnet50_fpn` で RetinaNet を使用してみることもできる。\n",
        "# SSDlite なら `torchvision.models.detection.ssdlite320_mobilenet_v3_large`\n",
        "# SSD は `torchvision.models.detection.ssd300_vgg16` を用いる\n",
        "# これらモデルの詳細については `https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html#instance-seg-output`\n",
        "\n",
        "# ヘルパライブラリをインポート\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "# #画像のデータ拡張 当面は不要\n",
        "# だが `get_transform()` で 用いているため試しに使ってみる\n",
        "# import albumentations as A\n",
        "# from albumentations.pytorch.transforms import ToTensorV2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xoj7FVT5b1KH"
      },
      "source": [
        "# 2. データセットの作成\n",
        "\n",
        "## 2.1 自作ライブラリの読み込み，下請け関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NwnrXEyyb1KH",
        "outputId": "c16b38a4-60a4-48ef-e9cb-18a4a37e325c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from bit import BIT\n",
        "\n",
        "if isColab:\n",
        "    bit = BIT(fontdata=notofonts)\n",
        "else:\n",
        "    bit = BIT()\n",
        "images, bboxes = bit.make_line_bisection_task_images(N=1, n_lines=3)\n",
        "\n",
        "#print(f'bboxes:{bboxes[0]}, len(bboxes):{len(bboxes[0])}')\n",
        "\n",
        "import PIL\n",
        "def plot_pilimg_and_bbox(pil_img:PIL.Image.Image, \n",
        "                         bboxes:list,\n",
        "                         verbose:bool=False\n",
        "                        ):\n",
        "    \"\"\"bounding box (物体を囲む四角形の境界領域のことを境界領域箱と呼ぶ): bbox\n",
        "    PIL 画像を境界領域と共に表示する関数\"\"\"\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.imshow(pil_img)\n",
        "    ax = plt.gca()\n",
        "    for (xmin, ymin, xmax, ymax), c in zip(bboxes, COLORS):\n",
        "        if verbose:\n",
        "            print(f'xmin:{xmin}, ymin:{ymin}')\n",
        "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                   fill=False, color=c, linewidth=2))    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSUDBRx0b1KI"
      },
      "source": [
        "## 2.2. 線分二等分線用画像の作成と書き出し"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kiY6H7qDb1KJ",
        "outputId": "30cb1752-43aa-4254-9ef3-8a5c12802e6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 19.2 s, sys: 267 ms, total: 19.4 s\n",
            "Wall time: 19.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# 訓練データセット，テストデータセットの作成\n",
        "\n",
        "#lines = [3]\n",
        "lines = [3,4,5]                # 一枚の刺激画像に何本の水平線が含まれるか\n",
        "#lines = [3,4,5,6]               # 一枚の刺激画像に何本の水平線が含まれるか\n",
        "#lines = [3,6]                   # 一枚の刺激画像に何本の水平線が含まれるか\n",
        "train_dups, test_dups = 10, 5  # 各条件ごとに何枚画像を生成するか\n",
        "#train_dups, test_dups = 25, 5  # 各条件ごとに何枚画像を生成するか\n",
        "#train_dups, test_dups = 5, 1    # 各条件ごとに何枚画像を生成するか\n",
        "train_bboxes, test_bboxes = [], []\n",
        "train_imgs, test_imgs = [], []\n",
        "\n",
        "# 訓練画像データ用ディレクトリが存在しなければ作成する\n",
        "if not os.path.exists('./data/2022bit_line_bisection'):\n",
        "    os.mkdir('./data/2022bit_line_bisection')\n",
        "train_dirname = './data/2022bit_line_bisection/train'\n",
        "test_dirname = './data/2022bit_line_bisection/test'\n",
        "if os.path.exists(train_dirname):\n",
        "    shutil.rmtree(train_dirname)\n",
        "if os.path.exists(test_dirname):\n",
        "    shutil.rmtree(test_dirname)\n",
        "\n",
        "for dir_name in [train_dirname, test_dirname]:\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.mkdir(dir_name) \n",
        "        \n",
        "for line in lines:\n",
        "    images, bboxes = bit.make_line_bisection_task_images(N=train_dups, n_lines=line)\n",
        "    train_imgs += images\n",
        "    train_bboxes += bboxes\n",
        "    images, bboxes = bit.make_line_bisection_task_images(N=test_dups, n_lines=line)\n",
        "    test_imgs += images\n",
        "    test_bboxes += bboxes\n",
        "\n",
        "# 訓練データセットの書き出し\n",
        "for i, img in enumerate(train_imgs):\n",
        "    stim_fname = f'{i:04d}.png'\n",
        "    stim_fname = os.path.join(train_dirname, stim_fname)\n",
        "    with open(stim_fname, 'wb') as fp:\n",
        "        img.save(fp,format='png')\n",
        "\n",
        "with open(os.path.join(train_dirname,'bboxes.txt'),'w') as fp:\n",
        "    for bbox in train_bboxes:\n",
        "        fp.write(str(bbox)+'\\n')\n",
        "            \n",
        "# 検証データセットの書き出し\n",
        "for i, img in enumerate(test_imgs):\n",
        "    stim_fname = f'{i:04d}.png'\n",
        "    stim_fname = os.path.join(test_dirname, stim_fname)\n",
        "    with open(stim_fname, 'wb') as fp:\n",
        "        img.save(fp,format='png')\n",
        "\n",
        "with open(os.path.join(test_dirname,'bboxes.txt'),'w') as fp:\n",
        "    for bbox in test_bboxes:\n",
        "        fp.write(str(bbox)+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCLXNyYxb1KJ"
      },
      "source": [
        "## 2.3 線分二等分線課題作成用データ作成クラスの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": [],
        "id": "m836KgKnb1KK"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import functional as F\n",
        "# import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "def get_transform():\n",
        "    \n",
        "    return A.Compose([\n",
        "        # ToTensorV2 converts image to pytorch tensor without div by 255\n",
        "        ToTensorV2(p=1.0) \n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "\n",
        "class BIT_LineBisection(torch.utils.data.Dataset):\n",
        "    \"\"\"留意事項\n",
        "    1. データセットはタプルを返す。1 つ目の要素は画像の形状，2 つ目の要素は辞書である。\n",
        "    2. 画像はデータセット定義時に指定したサイズでカラーモードは RGB\n",
        "    3. 画像には 4 つのバウンディングボックスがあり，これはボックス内の 4 つのリストとラベルの長さから明らかである。\n",
        "    \"\"\"\n",
        "    def __init__(self, dirname='data/2022bit_line_bisection/train',\n",
        "                 transforms=get_transform):\n",
        "        \n",
        "        self.transforms = transforms\n",
        "        self.dirname = dirname\n",
        "        self.data_fnames = sorted(glob(os.path.join(dirname,'*.png')))\n",
        "        #self.data_fnames = sorted(glob.glob(os.path.join(dirname,'*.png')))\n",
        "        self.n_data = len(self.data_fnames)\n",
        "\n",
        "        # 武藤先生から送信された画像は 4662 X 3289 なので，この縮尺因子で画像も境界ボックスも規格化する\n",
        "        muto_width = 4662\n",
        "        muto_height = 3289\n",
        "        self.height = 224\n",
        "        self.width = 224\n",
        "        self.height_f = 224 / muto_height\n",
        "        self.width_f =  224 / muto_width\n",
        "        #self.height_f *= int(muto_height / muto_width)\n",
        "        \n",
        "        # 境界領域ボックス (左,上,右,下) 4 点からなるデータを `bboxes.txt` から読み込む\n",
        "        bboxes_fname = os.path.join(self.dirname, 'bboxes.txt')\n",
        "        with open(bboxes_fname, 'r') as fp:\n",
        "            X = fp.readlines()\n",
        "        self.bboxes = {}\n",
        "        for i, line in enumerate(X):\n",
        "            digs = np.array([int(d) for d in line.strip().replace('[','').replace(']','').split(',')])\n",
        "            self.bboxes[i] = np.reshape(digs,((-1,4)))\n",
        "            \n",
        "            for box in self.bboxes[i]:\n",
        "                box[0] *= self.width_f\n",
        "                box[1] *= self.height_f\n",
        "                box[2] *= self.width_f\n",
        "                box[3] *= self.height_f\n",
        "            \n",
        "    def __getitem__(self, index):\n",
        "        #img = plt.imread(self.data_fnames[index])\n",
        "        #img = torch.Tensor(img).permute(2,0,1)\n",
        "        \n",
        "        # reading the images and converting them to correct size and color    \n",
        "        img = cv2.imread(self.data_fnames[index])\n",
        "        #img = cv2.imread(image_path)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
        "        # diving by 255\n",
        "        img_res /= 255.0\n",
        "        img = img_res\n",
        "        img = torch.Tensor(img).permute(2,0,1)\n",
        "        \n",
        "        # convert boxes into a torch.Tensor\n",
        "        bboxes = torch.as_tensor(self.bboxes[index], dtype=torch.float32)\n",
        "        \n",
        "        # getting the areas of the boxes\n",
        "        area = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
        "        \n",
        "         # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((bboxes.shape[0],), dtype=torch.int64)\n",
        "        \n",
        "        labels = torch.as_tensor([bit.symbols.index('<line>') for _ in range(self.bboxes[index].shape[0])], dtype=torch.int64)\n",
        "        #labels = torch.as_tensor([bit.symbols.index('<line>') for _ in range(train_dataset.bboxes[index].shape[0])], dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = bboxes\n",
        "        #target[\"labels\"] = labels.unsqueeze(0)\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        # image_id\n",
        "        image_id = torch.tensor([index])\n",
        "        target[\"image_id\"] = image_id        \n",
        "        \n",
        "        #transformed = self.transforms(image = img,\n",
        "        #                              bboxes = target['boxes'],\n",
        "        #                              labels = labels)\n",
        "        #img = transformed['image']\n",
        "        \n",
        "        return img, target\n",
        "        #return img, self.bboxes[index]\n",
        "        #return self.data_fnames[index], self.bboxes[index]\n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.n_data\n",
        "            \n",
        "train_dataset = BIT_LineBisection()\n",
        "test_dataset = BIT_LineBisection(dirname='data/2022bit_line_bisection/test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvz7vO5bb1KL"
      },
      "source": [
        "## 2.4. 作成した画像データを視覚化して例示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uvV7zECcb1KL",
        "outputId": "7421f51e-30d7-4cc8-89f1-c12041c880ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N:10 target:{'boxes': tensor([[ 23.,  65., 186.,  73.],\n",
            "        [ 29.,  97., 191., 106.],\n",
            "        [ 21., 131., 184., 140.],\n",
            "        [ 21., 162., 183., 172.],\n",
            "        [ 28., 189., 191., 198.]]), 'labels': tensor([1, 1, 1, 1, 1]), 'area': tensor([1304., 1458., 1467., 1620., 1467.]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'image_id': tensor([10])}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#N = np.random.choice(train_dataset.__len__())\n",
        "#img, target = train_dataset.__getitem__(N)\n",
        "N = np.random.choice(test_dataset.__len__())\n",
        "img, target = test_dataset.__getitem__(N)\n",
        "plot_pilimg_and_bbox(img.permute(1,2,0).detach().numpy(), target['boxes'])\n",
        "print(f'N:{N} target:{target}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKBUBIWFb1KL"
      },
      "source": [
        "## 2.5. PyTorch 用データセットの作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "olsu7CFIb1KL",
        "outputId": "ee8afcf3-bfac-4651-dbc5-e870d193040d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_dataset):30, len(test_dataset):15\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "# 学習・検証用データローダの定義 # define training and validation data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                           #batch_size=2,\n",
        "                                           batch_size=4, \n",
        "                                           shuffle=True, \n",
        "                                           num_workers=0,\n",
        "                                           collate_fn=utils.collate_fn)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                          batch_size=4, \n",
        "                                          shuffle=False, \n",
        "                                          num_workers=0,\n",
        "                                          collate_fn=utils.collate_fn)\n",
        "\n",
        "print(f'len(train_dataset):{len(train_dataset)}, len(test_dataset):{len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLVLP5jLb1KM"
      },
      "source": [
        "# 3. モデルの定義\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S33zt7Eb1KM"
      },
      "source": [
        "## 3.1 訓練済モデルの取得関数を定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "h-rA0po_b1KM",
        "outputId": "f5be7241-6695-4fc9-f16d-14fe6ce535db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "変換前 model.roi_heads:RoIHeads(\n",
            "  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
            "  (box_head): TwoMLPHead(\n",
            "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (box_predictor): FastRCNNPredictor(\n",
            "    (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
            "    (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
            "  )\n",
            ")\n",
            "変換後 model.roi_heads:RoIHeads(\n",
            "  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
            "  (box_head): TwoMLPHead(\n",
            "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (box_predictor): FastRCNNPredictor(\n",
            "    (cls_score): Linear(in_features=1024, out_features=49, bias=True)\n",
            "    (bbox_pred): Linear(in_features=1024, out_features=196, bias=True)\n",
            "  )\n",
            ")\n",
            "num_classes:49, bit.symbols:['<background>', '<line>', '★', 'あ', 'い', 'う', 'え', 'お', 'か', 'き', 'く', 'け', 'こ', 'さ', 'し', 'す', 'せ', 'そ', 'た', 'ち', 'つ', 'て', 'と', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ひ', 'ふ', 'へ', 'ほ', 'ま', 'み', 'む', 'め', 'も', 'や', 'ゆ', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'わ', 'を', 'ん']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RoIHeads(\n",
              "  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "  (box_head): TwoMLPHead(\n",
              "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "  )\n",
              "  (box_predictor): FastRCNNPredictor(\n",
              "    (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
              "    (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "def get_object_detection_model(num_classes):\n",
        "\n",
        "    # MS-COCO で事前に学習させたモデルを読み込み\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    #model = torchvision.models.mobilenet_v2(pretrained=True)\n",
        "    \n",
        "    #model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "    #model = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
        "    #retinanet_model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
        "    #print(retinanet_model.head)\n",
        "\n",
        "    #ssdlite_model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(pretrained=True)\n",
        "    #print(ssdlite_model.head.classification_head)\n",
        "    #print(ssdlite_model.head.regression_head)\n",
        "\n",
        "    # 分類器の入力特徴数の取得\n",
        "    #print(model.roi_heads) \n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    print(f'変換前 model.roi_heads:{model.roi_heads}')\n",
        "\n",
        "    # 事前学習済頭部を新しいものに置き換え\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
        "    print(f'変換後 model.roi_heads:{model.roi_heads}')\n",
        "\n",
        "    return model\n",
        "\n",
        "# 自作ヘルパー関数を使ってモデルの宣言\n",
        "num_classes = len(bit.symbols)\n",
        "model = get_object_detection_model(num_classes)\n",
        "print(f'num_classes:{num_classes}, bit.symbols:{bit.symbols}')\n",
        "#print(f'model:{model}')\n",
        "\n",
        "#model = torchvision.models.mobilenet_v2(pretrained=True)    \n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "#[(name, param.size()) for name, param in model.named_parameters()]\n",
        "model.roi_heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ykehul6b1KM"
      },
      "source": [
        "# 7. 訓練 <!-- # Training-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-K4w2gMb1KM"
      },
      "source": [
        "訓練用のモデルを準備\n",
        "<!-- Let's prepare the model for training -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0E-MkyQWb1KN"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "from collections import defaultdict\n",
        "from collections import deque\n",
        "\n",
        "def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n",
        "\n",
        "    def f(x):\n",
        "        if x >= warmup_iters:\n",
        "            return 1\n",
        "        alpha = float(x) / warmup_iters\n",
        "        return warmup_factor * (1 - alpha) + alpha\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
        "\n",
        "\n",
        "class SmoothedValue(object):\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"Warning: does not synchronize the deque!\"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median,\n",
        "            avg=self.avg,\n",
        "            global_avg=self.global_avg,\n",
        "            max=self.max,\n",
        "            value=self.value)\n",
        "\n",
        "\n",
        "class MetricLogger(object):\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
        "            type(self).__name__, attr))\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(\n",
        "                \"{}: {}\".format(name, str(meter))\n",
        "            )\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = ''\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}',\n",
        "                'max mem: {memory:.0f}'\n",
        "            ])\n",
        "        else:\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}'\n",
        "            ])\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time),\n",
        "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
        "                else:\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time)))\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print(f'{header} Total time:{total_time_str} ({total_time/len(iterable):.3f} s/it)')\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist              \n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "              \n",
        "def reduce_dict(input_dict, average=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dict (dict): all the values will be reduced\n",
        "        average (bool): whether to do average or sum\n",
        "    Reduce the values in the dictionary from all processes so that all processes\n",
        "    have the averaged results. Returns a dict with the same fields as\n",
        "    input_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return input_dict\n",
        "    with torch.no_grad():\n",
        "        names = []\n",
        "        values = []\n",
        "        # sort the keys so that they are consistent across processes\n",
        "        for k in sorted(input_dict.keys()):\n",
        "            names.append(k)\n",
        "            values.append(input_dict[k])\n",
        "        values = torch.stack(values, dim=0)\n",
        "        dist.all_reduce(values)\n",
        "        if average:\n",
        "            values /= world_size\n",
        "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
        "    return reduced_dict              "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from coco_utils import get_coco_api_from_dataset\n",
        "from coco_eval import CocoEvaluator\n",
        "#import utils\n",
        "import math\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
        "    model.train()\n",
        "    metric_logger = MetricLogger(delimiter=\": \")\n",
        "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.4f}'))\n",
        "    header = f'Epoch:[{epoch}]'\n",
        "\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1. / 1000\n",
        "        warmup_iters = min(1000, len(data_loader) - 1)\n",
        "\n",
        "        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        _images = list(image.to(device) for image in images)\n",
        "        _targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(_images, _targets)\n",
        "        losses = sum(_loss for _loss in loss_dict.values())\n",
        "\n",
        "        loss_dict_reduced = reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "        loss_value = losses_reduced.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(f\"損失値: {loss_value:.3f}, stopping training\")\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    return metric_logger\n",
        "\n",
        "def _get_iou_types(model):\n",
        "    model_without_ddp = model\n",
        "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "        model_without_ddp = model.module\n",
        "    iou_types = [\"bbox\"]\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
        "        iou_types.append(\"segm\")\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
        "        iou_types.append(\"keypoints\")\n",
        "    return iou_types\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    header = 'Test:'\n",
        "\n",
        "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "    iou_types = _get_iou_types(model)\n",
        "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        model_time = time.time()\n",
        "        outputs = model(images)\n",
        "\n",
        "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    return coco_evaluator\n"
      ],
      "metadata": {
        "id": "OO1ijZnVVlTD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Uv9LWpxBb1KN",
        "outputId": "eecc349a-156f-48d7-a43e-ae680b9bef50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "変換前 model.roi_heads:RoIHeads(\n",
            "  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
            "  (box_head): TwoMLPHead(\n",
            "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (box_predictor): FastRCNNPredictor(\n",
            "    (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
            "    (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
            "  )\n",
            ")\n",
            "変換後 model.roi_heads:RoIHeads(\n",
            "  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
            "  (box_head): TwoMLPHead(\n",
            "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (box_predictor): FastRCNNPredictor(\n",
            "    (cls_score): Linear(in_features=1024, out_features=49, bias=True)\n",
            "    (bbox_pred): Linear(in_features=1024, out_features=196, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# 可能なら GPU 上で学習させる\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "num_classes = len(bit.symbols)\n",
        "\n",
        "# 自作ヘルパー関数を使ってモデルを取得\n",
        "model = get_object_detection_model(num_classes)\n",
        "\n",
        "# モデルを正しいデバイスへ移動\n",
        "model.to(device)\n",
        "\n",
        "# 最適化関数の構成と設定  # construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(params, lr=0.001)\n",
        "\n",
        "# そして 3 エポックごとに学習率を 10 倍ずつ下げる学習率スケジューラ\n",
        "# and a learning rate scheduler which decreases the learning rate by 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QCSm3G6Ob1KN",
        "outputId": "1dc96b06-4e9b-4813-8340-927b62ffee2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475,
          "referenced_widgets": [
            "9d5fd57689714a2bb2fe934129bc77d6",
            "7ea16b09a321443b9debb11ce3384748",
            "1da54e195f0e4f0e9778bea012558e56",
            "21309002a7564b1593236c1a2bff6ed7",
            "4af6f761774b4e13a343d45a61880449",
            "2b2edf391d6d44e6b67ecfd00c6b0e9f",
            "9b4c19728f684217a3e68aeafaf7e798",
            "3ac5845e806f441c89fb3ec67468ce38",
            "f58a53c4b2744b8eb1fcac45b4a027c9",
            "d5b6c3911a28445f88e3ee971246c828",
            "19b1f1830095461fbc544d07b91c1edb"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "エポック:1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d5fd57689714a2bb2fe934129bc77d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-9c7e1e2c989a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#print(f'_targets:{_targets}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m#loss = model(images, targets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     93\u001b[0m                     )\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/backbone_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mout_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/ops/misc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 158.00 MiB (GPU 0; 14.76 GiB total capacity; 13.36 GiB already allocated; 93.75 MiB free; 13.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# 10 エポック分訓練 # training for 10 epochs\n",
        "#num_epochs = 10\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'エポック:{epoch+1}')\n",
        "\n",
        "    model.train()\n",
        "    for images, targets in tqdm(train_loader):\n",
        "        \n",
        "        _images = list(image.to(device) for image in images)\n",
        "        _targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        #print(f'_targets:{_targets}')\n",
        "        loss = model(_images, _targets)\n",
        "        #loss = model(images, targets)\n",
        "    \n",
        "    # 学習率の更新\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    #テストデータセットで評価\n",
        "    evaluate(model, test_loader, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CMMPMjnb1KO"
      },
      "source": [
        "# 9. モデルの検証 <!-- # Testing our Model -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PrK8hFCb1KO"
      },
      "outputs": [],
      "source": [
        "# 検証セットから画像を 1 枚選ぶ\n",
        "N = np.random.choice(len(test_dataset))\n",
        "img, target = test_dataset[N]\n",
        "\n",
        "model.eval() # モデルを評価モードにする\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])[0]\n",
        "    \n",
        "print(f'予測 {N:2d} バウンディングボックス数:{len(prediction[\"labels\"])}')\n",
        "print(f'実際のバウンディングボックス数    :{len(target[\"labels\"])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDHiIBl1b1KO"
      },
      "source": [
        "プロットして，何が予測されたかをチェック"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5v9_7esb1KO"
      },
      "outputs": [],
      "source": [
        "plot_img_bbox(torch_to_pil(img), target, title='グランドトルース')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NRIosu0b1KO"
      },
      "outputs": [],
      "source": [
        "plot_img_bbox(torch_to_pil(img), prediction, title='モデル予測')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcz_v16Mb1KO"
      },
      "source": [
        "このモデルは，すべてのリンゴについて多くのバウンディングボックスを予測していることがわかります。\n",
        "これに nms を適用し，最終的な出力を見てみましょう。\n",
        "<!-- You can see that our model predicts a lot of bounding boxes for every apple. Lets apply nms to it and see the final output -->"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# この関数は，元の予測値と iou の閾値を受け取ります\n",
        "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
        "    # torchvision は保持すべき bbox のインデックスを返します。\n",
        "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
        "    \n",
        "    final_prediction = orig_prediction\n",
        "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
        "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
        "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
        "    return final_prediction\n",
        "\n",
        "def torch_to_pil(img):\n",
        "    \"\"\"torchtensor を PIL 画像に変換する関数\"\"\"\n",
        "    return torchtrans.ToPILImage()(img).convert('RGB')"
      ],
      "metadata": {
        "id": "EufupSYAWHv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSI2m2Oob1KP"
      },
      "outputs": [],
      "source": [
        "nms_prediction = apply_nms(prediction, iou_thresh=0.2)\n",
        "print('NMS APPLIED MODEL OUTPUT')\n",
        "plot_img_bbox(torch_to_pil(img), nms_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpufGtSbb1KP"
      },
      "source": [
        "では，テストセットから画像を取り出し，予測してみましょう。\n",
        "<!-- Now lets take an image from the test set and try to predict on it -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLVBrZxgb1KP"
      },
      "outputs": [],
      "source": [
        "test_dataset = FruitImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n",
        "\n",
        "# テストセットから画像を 1 枚選ぶ\n",
        "# pick one image from the test set\n",
        "#img, target = test_dataset[10]\n",
        "img, target = test_dataset[np.random.choice(len(test_dataset))]\n",
        "\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])[0]\n",
        "    \n",
        "#print('期待される出力\\n')\n",
        "# print('EXPECTED OUTPUT\\n')\n",
        "plot_img_bbox(torch_to_pil(img), target, title=\"グランドトルース\")\n",
        "\n",
        "#print('モデルの出力\\n')\n",
        "# print('MODEL OUTPUT\\n')\n",
        "nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n",
        "\n",
        "plot_img_bbox(torch_to_pil(img), nms_prediction, title=\"モデル予測\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrKBvSqbb1KP"
      },
      "source": [
        "このモデルは，単一物体の画像ではよくできています。\n",
        "<!-- The model does well on single object images.-->\n",
        "\n",
        "モデルはスライスも予測していることがわかりますが，これは失敗を意味します☹️ 。\n",
        "しかし心配しないでください。\n",
        "これは単なる基本的なモデルであり，我々がそれを改善できるいくつかのアイデアがあります:\n",
        "<!-- You can see that our model predicts the slices too and that means a failure ☹️ . But fear not, this is just a base line model here are some ideas we can improve it -  -->\n",
        "\n",
        "1. より良いモデルを使用する。\n",
        "現在使用しているモデルのバックボーンである `resnet50` を変更し，微調整を行うことが可能です。\n",
        "2. 画像の大きさ，最適化関数，学習速度のスケジュールなどの学習設定を変更することができる。\n",
        "3. データ拡張を追加することができる。\n",
        "ここでは，データ拡張関数の豊富なライブラリを持つ Albumentations ライブラリを使用しました。\n",
        "自由に探索し，試してみてください。\n",
        "\n",
        "<!-- 1. Use a better model. \n",
        "   We have the option of changing the backbone of our model which at present is `resnet 50` and the fine tune it.\n",
        "2. We can change the training configurations like size of the images, optimizers and learning rate schedule.\n",
        "3. We can add more augmentations.\n",
        "   We have used the Albumentations library which has an extensive library of data augmentation functions. Feel free to explore and try them out.  -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G9Aczlob1KP"
      },
      "source": [
        "# 10. 結果の保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh1bFLsZb1KP"
      },
      "outputs": [],
      "source": [
        "torch.save({'model': model.state_dict(),\n",
        "            'optim': optimizer.state_dict()\n",
        "           }, '2022_0318fine_tune_fruit.cpt')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "2022_0604bit_faster-rcnn_fine_tuning.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9d5fd57689714a2bb2fe934129bc77d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ea16b09a321443b9debb11ce3384748",
              "IPY_MODEL_1da54e195f0e4f0e9778bea012558e56",
              "IPY_MODEL_21309002a7564b1593236c1a2bff6ed7"
            ],
            "layout": "IPY_MODEL_4af6f761774b4e13a343d45a61880449"
          }
        },
        "7ea16b09a321443b9debb11ce3384748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b2edf391d6d44e6b67ecfd00c6b0e9f",
            "placeholder": "​",
            "style": "IPY_MODEL_9b4c19728f684217a3e68aeafaf7e798",
            "value": "  0%"
          }
        },
        "1da54e195f0e4f0e9778bea012558e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ac5845e806f441c89fb3ec67468ce38",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f58a53c4b2744b8eb1fcac45b4a027c9",
            "value": 0
          }
        },
        "21309002a7564b1593236c1a2bff6ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5b6c3911a28445f88e3ee971246c828",
            "placeholder": "​",
            "style": "IPY_MODEL_19b1f1830095461fbc544d07b91c1edb",
            "value": " 0/8 [00:00&lt;?, ?it/s]"
          }
        },
        "4af6f761774b4e13a343d45a61880449": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b2edf391d6d44e6b67ecfd00c6b0e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b4c19728f684217a3e68aeafaf7e798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ac5845e806f441c89fb3ec67468ce38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f58a53c4b2744b8eb1fcac45b4a027c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5b6c3911a28445f88e3ee971246c828": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19b1f1830095461fbc544d07b91c1edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}